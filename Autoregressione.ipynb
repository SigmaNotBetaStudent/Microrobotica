{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "285cca39-3b5b-4a73-b9de-44afcc8bd8a0",
   "metadata": {},
   "source": [
    "# Un modello matematico del testo in linguaggio naturale\n",
    "\n",
    "Questo modello ci aiuterà a capire come funzionano i **LLM** (Large Language Model)\n",
    "\n",
    "* I LLM sono le reti neurali alla base dell'IA generativa\n",
    "\n",
    "* I LLM generano il testo mediante un oricessi detti **autoregressione**: quando forniamo un prompt all'IA generativa, il LLM genera la risposta **token** per **token**. A ogni istante della generazione della risposta, LLM prende in input tutti i token del prompt e tutti i nuovi token che ha generato fino a quel momento.\n",
    "\n",
    "* **token**= 0.175 parole\n",
    "\n",
    "* **autoregressione**= \"regressione\": generare qualcosa di nuovo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90aa958d-35fc-4519-9ab7-09fa576ae853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importazioni\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d95cd650-ec54-408a-9e95-6b5a0432c724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caricamento input\n",
    "file = open(\"./sei_personaggi_in_cerca_dautore_Luigi_Pirandello.txt\", encoding=\"utf-8\")\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a26de69-1897-46ee-99ea-79e7c83be130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pre-processing\n",
    "text = text.replace(\"sigma\", \" \").lower()\n",
    "for _ in range(2):\n",
    "    text = text.replace(\"  \", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55bfae2c-e3af-454e-927a-eb551f0d92bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I nostri token sono sempre un carattere\n",
    "l_context = 6\n",
    "markov_dict = {}\n",
    "for i,char in enumerate(text[:-l_context]):\n",
    "    ngram = text[i:i+l_context] #sequenza di n lettere\n",
    "    if ngram in markov_dict:\n",
    "        markov_dict[ngram].append(text[i+l_context])\n",
    "    else:\n",
    "        markov_dict[ngram] = [text[i+l_context]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a786647-7d52-4ad0-b859-43a383e748c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sei ca'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m markov_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msei ca\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sei ca'"
     ]
    }
   ],
   "source": [
    "markov_dict[\"sei ca\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e389e-a34f-493b-a5f4-3d8ab7b41437",
   "metadata": {},
   "source": [
    "Il dizionario `markov_dict` è un modello matematico del linguaggio naturale che ora usiamo per costruire un autoregressone Markoviano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "364d100c-1feb-4d6b-82c3-0abad63824a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sei personaggi\\n\\nnon abbia pazienza un momento, perché per il fastidio, perché non ha\\nragioni,\\nuna tra lo s'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_0 = \"sei pe\"\n",
    "l_phrase = 100\n",
    "phrase= ngram_0\n",
    "if len(ngram_0) != l_context:\n",
    "    print(\"Errore!\")\n",
    "else:\n",
    "    phrase = ngram_0\n",
    "    for _ in range(l_phrase):\n",
    "        if ngram_0 in markov_dict:\n",
    "            next_char = random.choice(markov_dict[ngram_0])\n",
    "            phrase += next_char\n",
    "            ngram_0 = phrase[-l_context:]\n",
    "        else:\n",
    "            break\n",
    "phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9794a8ef-1d9c-419a-93a6-b88a0cd5dcd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf973398-0b84-43bc-abe8-494c02ab5ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
